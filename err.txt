[2023-10-27 14:20:36,883] torch.distributed.run: [WARNING] 
[2023-10-27 14:20:36,883] torch.distributed.run: [WARNING] *****************************************
[2023-10-27 14:20:36,883] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2023-10-27 14:20:36,883] torch.distributed.run: [WARNING] *****************************************
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/transformers/training_args.py:1598: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model initialized on CPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.87s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.91s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.89s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.93s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.86s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:34<01:09, 34.87s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:46<01:33, 46.69s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.35s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:10<00:35, 35.36s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:19<00:38, 38.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.59s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.60s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.60s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.61s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.89s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.59s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.60s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 28.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:31<00:00, 30.59s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards: 100%|██████████| 3/3 [01:36<00:00, 28.91s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:36<00:00, 32.30s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Constructing dataset...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
WARNING:root:Tokenizing inputs... This may take some time...
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
WARNING:root:Tokenizing inputs... This may take some time...
Traceback (most recent call last):
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 61, in <module>
    train()
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 54, in train
    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)
  File "/lustre/rganti/fmaas-tuning/tuning/trainer.py", line 13, in make_supervised_data_module
    train_dataset = tokenizer_data_utils.SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 93, in __init__
    data_dict = preprocess(sources, targets, tokenizer)
  File "/lustre/rganti/fmaas-tuning/tuning/data/tokenizer_data_utils.py", line 66, in preprocess
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in tqdm(examples, sources)]
TypeError: 'module' object is not callable
WARNING:root:Constructing dataset...
[2023-10-27 14:22:36,996] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2515028 closing signal SIGTERM
[2023-10-27 14:22:38,262] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2515029) of binary: /home/rganti/anaconda3/envs/tuning/bin/python
Traceback (most recent call last):
  File "/home/rganti/anaconda3/envs/tuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/rganti/anaconda3/envs/tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tuning/trainer.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2515030)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 2515031)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2515032)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 2515033)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 2515034)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 2515035)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-10-27_14:22:36
  host      : a-100-st-p4de24xlarge-25.a100-slingshot.pcluster
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2515029)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
